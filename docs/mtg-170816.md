## Meeting notes Aug 16, 2017

Meeting: Lauren, Andrea, Noah, MH

### Immediate next steps
  - Spot check CoreNLP analysis
  - Does the language look different by property distribution?
  - Does the language look different by whether or not partner did well?
  - For each distribution, what is the random baseline?
  - Make a list of *things we can hope to be able to count from text*

### Ideas for Data analysis

  - We can ignore the actual words produced, and use the behavior (accuracy) to make inference about efficiency of language (perhaps in comparison to single player)
  - What language do people use?
  - Can we analyze the transcripts at the level of sentences?
    - Does CoreNLP parse the sentences correctly? (Spot check, see above)
    - Does Sentence length
  - Is there a simple regular expression for picking out generics?
    - Judith and MH have tried to extract generics from corpora (but those corpora are tagged using Penn Treebank)
  - Language of theoretical interest: Quantifiers, generics, numbers
  - Study time, when learning from examples is self-paced
    - Can compare this with spent talking (perhaps we should lift the requirement that participants have to wait 30s in the chatroom)
    - Can look at performance as a function of how much time it takes
      - Language should be faster than learning from examples
  - Language learning through RSA
    - Could follow-up on the work of Katherine & Bill
    - Could learn an intermediate representation (like a rule in Rational Rules), try to learn the rules that correspond to the utterances
    - Or don't learn an intermediate representation (better for learning algorithms; can do end-to-end training)

### Ideas for Further Studies
  - Designs for which a partner's performance is worse than a single player (individual learning)?
  - Half trials you are tested on what you're trained on; half you're tested on what your partner is trained on.
  - Are there some concepts that are hard to communicate (but that are still learnable)?
    - Implications for cultural ratchet.
      - Cultural evolution should not ratchet the concepts that can't be communicated easily.
      - Perhaps these concepts are more likely to alter genetic evolution
  - Kinds of concepts
    - "Information integration" concepts (e.g., an integration of two-dimensions)
  - Boolean concept learning: Predict name (fep or not)?
    - When you click, find out fep or not. Objects vary along some number of (binary) dimensions.
    - Is there some relation between language produced and the probabilistic language of thought rules?
    - Hard concepts could be good for collaborative learning
      - Single subject, trained on X data.
      - Single subject, trained on 2X data.
      - Two subjects: each trained on X and can talk.

### Further afield
  - Try to connect language to the language of thought (maybe with Steve P.'s help)
  - CogSci Paper(s) (est. Dec 2017); Journal Paper(s) (est. May 2018)
    - Humans know concepts. People study concept learning from examples.
    - There have been various stories about inductive learning.
    - Actual humans get conceptual knowledge from not examples but from instructions. Learning concepts from language.
    - We can use concepts similar to standard canon of artificial concept learning.
      - How efficient is the communication?
      - Qualitative analysis of language.
      - Better yet: Quantitative model.
        - Possibly connecting language of thought to natural language
          - Take a LoT used in rational rules. Do inference. Pick out high posterior rules. Treat language channel model as a translation from the rules to the words people say.
        - Which concepts are differentially easy to learn from language vs. examples?
          - Might get a noise process that degrades the rules and then passes it through the channel.
      - Learning
        - Standard concepts from rational rules (maybe 7-feature concepts from Feldman)
        - Example from [Logical primitives of thought](https://colala.bcs.rochester.edu/papers/piantadosi2016logical.pdf)


#### Also, following a meeting later that day with Lauren, Andrea and MH.
- In Pilot 1: Some of our dyads are using specific language (e.g., 2 of the 4 wugs lay eggs)
  - This may come from the belief that the exemplars presented are the only exemplars of the category
  - Some language is ambiguous (e.g., "Half lay eggs" could mean "In general, half" or "Half of the 4 lay eggs")
- For future versions
  - Make the number of exemplars during training different from the number of exemplars during test
  - Perhaps make the number of exemplars per species stochastic (could be 7 lorches, 9 feps, 10 wugs)
